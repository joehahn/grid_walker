{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_walker.ipynb\n",
    "#\n",
    "#by Joe Hahn\n",
    "#jmh.datasciences@gmail.com\n",
    "#3 February 2018\n",
    "#\n",
    "#grid_walker uses Q-learning to teach a neural net AI how to navigate an agent about\n",
    "#a 6x6 grid, guiding it towards a goal while avoiding obstacles and hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game settings\n",
    "grid_size = 6             #the grid_walker game spans a 6x6 grid of cells\n",
    "init = 'random_agent'     #the grid_walker agent is to be placed in a random location in this grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed random number generator\n",
    "rn_seed = 15\n",
    "import numpy as np\n",
    "np.random.seed(rn_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the grid_walker game\n",
    "from grid_walker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': [0, 1, 2, 3],\n",
       " 'acts': ['up', 'down', 'left', 'right'],\n",
       " 'grid_size': 6,\n",
       " 'init': 'random_agent',\n",
       " 'max_moves': 36,\n",
       " 'objects': ['agent', 'goal', 'pit', 'wall']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the environment, which is just a dictionary that contains all the\n",
    "#parameters describing the grid_walker game\n",
    "environment = initialize_environment(grid_size, init)\n",
    "environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': {'x': 0, 'y': 5},\n",
       " 'goal': {'x': 4, 'y': 4},\n",
       " 'pit': {'x': 4, 'y': 2},\n",
       " 'wall': {'x': 1, 'y': 4}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate the system's initial state, which is another dict containing\n",
    "#the x,y coordinates of all objects in the system\n",
    "state = initialize_state(environment)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent', 'goal', 'pit', 'wall']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This system is comprised of 4 objects. The locations of the goal, pit, and wall are constant\n",
    "#while the agent is mobile and can roam about the 6x6 grid\n",
    "objects = environment['objects']\n",
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions =  [0, 1, 2, 3]\n",
      "acts =  ['up', 'down', 'left', 'right']\n"
     ]
    }
   ],
   "source": [
    "#the agent has 4 possible actions: it can move up when action=0, down (action=1),\n",
    "#left (action=2), or right (action=3)\n",
    "actions = environment['actions']\n",
    "acts = environment['acts']\n",
    "print 'actions = ', actions\n",
    "print 'acts = ', acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' '' 'A' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      "state:\n",
      "     {'wall': {'y': 4, 'x': 1}, 'pit': {'y': 2, 'x': 4}, 'goal': {'y': 4, 'x': 4}, 'agent': {'y': 3, 'x': 3}}\n"
     ]
    }
   ],
   "source": [
    "#a 6x6 numpy array is used to indicte the x,y locations of agent A, wall W, pit P, and goal G.\n",
    "#Note that a nmpy array printed to the screen will appear to have its x,y axes flipped,\n",
    "#so a transpose and rotation of the grid array is used to undo that flip.\n",
    "#Here the agent is also manually placed at x=3,y=3 for convenience\n",
    "state['agent'] = {'x':3, 'y':3}\n",
    "grid = make_grid(state, environment)\n",
    "print np.rot90(grid.T)\n",
    "print 'state:'\n",
    "print '    ', state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act =  right\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' '' '' 'A' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      "next_state:\n",
      "     {'wall': {'y': 4, 'x': 1}, 'pit': {'y': 2, 'x': 4}, 'goal': {'y': 4, 'x': 4}, 'agent': {'y': 3, 'x': 4}}\n",
      "reward =  -1\n",
      "game_state =  running\n"
     ]
    }
   ],
   "source": [
    "#Now move the agent one grid-cell to the right, and note that this move generated a reward of -1:\n",
    "action = 3\n",
    "print 'act = ', acts[action]\n",
    "next_state = move_agent(state, action, environment)\n",
    "grid = make_grid(next_state, environment)\n",
    "print np.rot90(grid.T)\n",
    "print 'next_state:'\n",
    "print '    ', next_state\n",
    "print 'reward = ', get_reward(next_state, state)\n",
    "state = next_state\n",
    "N_moves = 1\n",
    "print 'game_state = ', get_game_state(state, N_moves, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act =  up\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' '*' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      "next_state:\n",
      "     {'wall': {'y': 4, 'x': 1}, 'pit': {'y': 2, 'x': 4}, 'goal': {'y': 4, 'x': 4}, 'agent': {'y': 4, 'x': 4}}\n",
      "reward =  10\n",
      "game_state =  goal\n"
     ]
    }
   ],
   "source": [
    "#walk the agent up to goal G (which is then displayed as *), that move generates a reward of 10\n",
    "#Note that the game_state has now changed to 'goal' to signify the end of a game.\n",
    "action = 0\n",
    "print 'act = ', acts[action]\n",
    "next_state = move_agent(state, action, environment)\n",
    "grid = make_grid(next_state, environment)\n",
    "print np.rot90(grid.T)\n",
    "print 'next_state:'\n",
    "print '    ', next_state\n",
    "print 'reward = ', get_reward(next_state, state)\n",
    "state = next_state\n",
    "N_moves += 1\n",
    "print 'game_state = ', get_game_state(state, N_moves, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act =  down\n",
      "act =  down\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '@' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      "next_state:\n",
      "     {'wall': {'y': 4, 'x': 1}, 'pit': {'y': 2, 'x': 4}, 'goal': {'y': 4, 'x': 4}, 'agent': {'y': 2, 'x': 4}}\n",
      "reward =  -10\n"
     ]
    }
   ],
   "source": [
    "#Now walk the agent into pit P (now displayed as @) and generate reward of -10\n",
    "action = 1\n",
    "print 'act = ', acts[action]\n",
    "next_state = move_agent(state, action, environment)\n",
    "state = next_state\n",
    "print 'act = ', acts[action]\n",
    "next_state = move_agent(state, action, environment)\n",
    "grid = make_grid(next_state, environment)\n",
    "print np.rot90(grid.T)\n",
    "print 'next_state:'\n",
    "print '    ', next_state\n",
    "print 'reward = ', get_reward(next_state, state)\n",
    "state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the grid_walker game does not let the agent walk into wall W or beyond the 6x6 grid,\n",
    "#and such moves generate a reward of -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A grid_walker game terminates when the agent walks into goal G, pit P, or walks for more\n",
    "#than grid_size**2 = 36 moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 36)                108       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 148       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,588\n",
      "Trainable params: 1,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#assemble neural network that agent will use to navigate the grid. The simple neural\n",
    "#network used here has two dense hidden layers having grid_size**2 = 36 neurons in each layer\n",
    "#When trained, this model uses epsilon-greedy Q-learning to estimate which of the agent's\n",
    "#four possible moves (up down left or right) that best maximizes the agent's future rewards.\n",
    "state_vector = state2vector(state, environment)\n",
    "N_inputs = state_vector.shape[1]\n",
    "N_outputs = len(actions)\n",
    "model = build_model(N_inputs, grid_size, N_outputs)\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "training done.\n"
     ]
    }
   ],
   "source": [
    "#Use Q-learning to train neural network to walk agent A towards goal G...for details see\n",
    "#the train() function in grid_walker.py...about two minutes to execute.\n",
    "N_training_games = 1000                #number of games to play while training model\n",
    "gamma = 0.85                           #discount for future rewards\n",
    "memories_size = 300                    #size of memory queue size, for experience replay\n",
    "batch_size = memories_size/15          #number of memories to use when fitting the model\n",
    "debug = False                          #set to True to grid the last move of every training game\n",
    "print 'training model...'\n",
    "trained_model = train(environment, model, N_training_games, gamma, memories_size, batch_size, debug=debug)\n",
    "print 'training done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state:\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' 'A' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      "=======================\n",
      " move : 1    action: up\n",
      "reward: -1\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' 'A' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      " move : 2    action: up\n",
      "reward: -1\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' 'A' '' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      " move : 3    action: right\n",
      "reward: -1\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' 'G' '']\n",
      " ['' '' '' 'A' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      " move : 4    action: up\n",
      "reward: -1\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' 'A' 'G' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      " move : 5    action: right\n",
      "reward: 10\n",
      "[['' '' '' '' '' '']\n",
      " ['' 'W' '' '' '*' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' 'P' '']\n",
      " ['' '' '' '' '' '']\n",
      " ['' '' '' '' '' '']]\n",
      "game_state: goal\n"
     ]
    }
   ],
   "source": [
    "#test the trained neural network by using it to guide the Agent from an initial random location to goal G,\n",
    "#with each step gridded along the way\n",
    "display_stats = True\n",
    "initial_state, final_state, N_moves, game_state = test_model(trained_model, environment, display_stats=display_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run multiple tests until agent has started from all possible starting positions\n",
    "display_stats = False\n",
    "state = initialize_state(environment)\n",
    "game_states = make_grid(state, environment)\n",
    "while (game_states == '').sum():\n",
    "    initial_state, final_state, N_moves, game_state = \\\n",
    "        test_model(trained_model, environment, display_stats=display_stats)\n",
    "    agent = initial_state['agent']\n",
    "    x = agent['x']\n",
    "    y = agent['y']\n",
    "    game_states[y, x] = game_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['g' 'g' 'g' 'g' 'g' 'g']\n",
      " ['m' 'W' 'g' 'g' 'G' 'g']\n",
      " ['g' 'g' 'g' 'g' 'g' 'g']\n",
      " ['g' 'g' 'g' 'g' 'P' 'g']\n",
      " ['g' 'g' 'g' 'g' 'p' 'p']\n",
      " ['g' 'g' 'g' 'g' 'g' 'g']]\n"
     ]
    }
   ],
   "source": [
    "#cells containing 'g' indicate that agent successfully navigated to Goal from those starting points,\n",
    "#while cells containing 'm' indicate where agent fails to find Goal before game ends due to 'max_moves',\n",
    "#while 'p' means agent bundered into pit P from those initial cells.\n",
    "print np.rot90(game_states.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "[['g' 'g' 'g' 'g' 'g' 'g']\n",
      " ['g' 'W' 'g' 'g' 'G' 'g']\n",
      " ['g' 'g' 'g' 'g' 'g' 'g']\n",
      " ['g' 'g' 'g' 'g' 'P' 'g']\n",
      " ['g' 'g' 'g' 'g' 'g' 'g']\n",
      " ['g' 'g' 'g' 'g' 'g' 'g']]\n"
     ]
    }
   ],
   "source": [
    "#rebuild, retrain, and retest the model\n",
    "model = build_model(N_inputs, grid_size, N_outputs)\n",
    "N_training_games = 3000\n",
    "print 'training...'\n",
    "trained_model = train(environment, model, N_training_games, gamma, memories_size, batch_size, debug=debug)\n",
    "state = initialize_state(environment)\n",
    "game_states = make_grid(state, environment)\n",
    "while (game_states == '').sum():\n",
    "    initial_state, final_state, N_moves, game_state = \\\n",
    "        test_model(trained_model, environment, display_stats=display_stats)\n",
    "    agent = initial_state['agent']\n",
    "    x = agent['x']\n",
    "    y = agent['y']\n",
    "    game_states[y, x] = game_state[0]\n",
    "print np.rot90(game_states.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
